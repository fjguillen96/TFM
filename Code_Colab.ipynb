{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Code.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMS7QmQZE64L"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import History\n",
        "import time\n",
        "from google.colab import files\n",
        "import io"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYRCHjzWE64Q"
      },
      "source": [
        "start_time = time.time()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHDrLH74E64Q"
      },
      "source": [
        "Se establece \"low_memory=False\", pues hay un problema dtype"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zipfTqXIE64Q"
      },
      "source": [
        "\n",
        "#cargar datos con la URL del raw de GitHub\n",
        "url = 'https://raw.githubusercontent.com/fjguillen96/TFM/main/train.csv'\n",
        "df = pd.read_csv(url , low_memory=False)\n",
        "\n",
        "\n",
        "\n",
        "#df_2 = pd.read_csv('test.csv' , low_memory=False)\n",
        "#Id = df_2['Id']\n",
        "#percent = df_2.shape[0]/(df_1.shape[0]+df_2.shape[0])\n",
        "#df_1 = df_1.drop(['Customers'],axis = 1)\n",
        "#df_2 = df_2.drop(['Id'],axis = 1)\n",
        "#df = pd.concat([df_1,df_2], axis = 0)\n",
        "#print(df_1.shape[0])\n",
        "#print(round(percent,5))\n",
        "#print((1-round(percent,7)) * df.shape[0])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo6GEsNQE64R"
      },
      "source": [
        "store = pd.read_csv('store.csv')\n",
        "print(store.shape)\n",
        "store.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzO_wfFPE64R"
      },
      "source": [
        "\"on = 'Store'\" establece que se buscarán los valores comunes en dicha columna, y no en parejas o tríos de características iguales.\n",
        "\"how='inner'\" establece que se buscarán valores comunes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TurfSfZgE64S"
      },
      "source": [
        "df_new = df.merge(store,on = ['Store'], how='inner')\n",
        "print(df_new.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF-IfVQZE64S"
      },
      "source": [
        "print(\"Distinct number of Stores :\", len(df_new[\"Store\"].unique()))\n",
        "print(\"Distinct number of Days :\", len(df_new[\"Date\"].unique()))\n",
        "print(\"Average daily sales of all stores : \",round(df_new[\"Sales\"].mean(),2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKw8qKrGE64S"
      },
      "source": [
        "## Extraemos los datos temporales de las fechas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFMiFYsKE64S"
      },
      "source": [
        "df_new['Date'] = pd.to_datetime(df_new['Date'], infer_datetime_format=True)\n",
        "df_new[\"Month\"] = df_new[\"Date\"].dt.month\n",
        "df_new[\"Quarter\"] = df_new[\"Date\"].dt.quarter\n",
        "df_new[\"Year\"] = df_new[\"Date\"].dt.year\n",
        "df_new[\"Day\"] = df_new[\"Date\"].dt.day\n",
        "df_new[\"Week\"] = df_new[\"Date\"].dt.isocalendar().week\n",
        "\n",
        "df_new[\"Season\"] = np.where(df_new[\"Month\"].isin([3,4,5]),\"Spring\",\n",
        "np.where(df_new[\"Month\"].isin([6,7,8]),\"Summer\",\n",
        "np.where(df_new[\"Month\"].isin([9,10,11]),\"Fall\",\n",
        "np.where(df_new[\"Month\"].isin([12,1,2]),\"Winter\",\"None\"))))\n",
        "\n",
        "print(df_new[[\"Date\",\"Year\",\"Month\",\"Day\",\"Week\",\"Quarter\",\"Season\"]].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq-pnPKKE64T"
      },
      "source": [
        "## Predicción"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddSw4qlyE64T"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.hist(df_new[\"Sales\"])\n",
        "plt.title(\"Histogram for Store Sales\")\n",
        "plt.xlabel(\"bins\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2lratS0E64T"
      },
      "source": [
        "Sacamos los nulos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFe0b6dgE64T"
      },
      "source": [
        "df_new.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOciXZcNE64U"
      },
      "source": [
        "Lo pasamos a porcentaje para visualizarlo mejor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2rBZ69OE64U"
      },
      "source": [
        "df_new.isnull().sum()/df_new.shape[0] * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyijntx-E64U"
      },
      "source": [
        "df_new[\"CompetitionDistance\"]=df_new[\"CompetitionDistance\"].fillna(df_new[\"CompetitionDistance\"].mode()[0])\n",
        "df_new[\"CompetitionDistance\"].isnull().sum()/df_new.shape[0] * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JegJrra6E64U"
      },
      "source": [
        "## Más visualización para entender los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nElB8ew6E64U"
      },
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "ax = sns.barplot(x=\"Season\", y=\"Sales\", data=df_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ide3OexE64V"
      },
      "source": [
        "ax = sns.barplot(x=\"Assortment\", y=\"Sales\", data=df_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5K28XrME64V"
      },
      "source": [
        "ax = sns.barplot(x=\"StoreType\", y=\"Sales\", data=df_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIr1kHExE64V"
      },
      "source": [
        "ax = sns.barplot(x=\"Season\", y=\"Sales\", data=df_new,estimator=np.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn87OYVZE64V"
      },
      "source": [
        "ax = sns.barplot(x=\"Assortment\", y=\"Sales\", data=df_new,estimator=np.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUOhh3RmE64V"
      },
      "source": [
        "ax = sns.barplot(x=\"StoreType\", y=\"Sales\", data=df_new,estimator=np.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiWDAcSyE64W"
      },
      "source": [
        "## Pasamos a variable dummy de una forma distinta (Manual)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPmym4LuE64W"
      },
      "source": [
        "target = [\"Sales\"]\n",
        "numeric_columns = [\"Customers\",\"Open\",\"Promo\",\"Promo2\",\n",
        "\"StateHoliday\",\"SchoolHoliday\",\"CompetitionDistance\"]\n",
        "categorical_columns = [\"DayOfWeek\",\"Quarter\",\"Month\",\"Year\",\n",
        "\"StoreType\",\"Assortment\",\"Season\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9k7vW57E64W"
      },
      "source": [
        "def create_ohe(df, col):\n",
        "    \n",
        "    le = LabelEncoder()\n",
        "    a=le.fit_transform(df[col]).reshape(-1,1)\n",
        "    ohe = OneHotEncoder(sparse=False)\n",
        "    column_names = [col+ \"_\"+ str(i) for i in le.classes_]\n",
        "\n",
        "    return(pd.DataFrame(ohe.fit_transform(a),columns =column_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQWDkjF6E64W"
      },
      "source": [
        "temp = df_new[numeric_columns]\n",
        "for column in categorical_columns:\n",
        "    temp_df = create_ohe(df_new,column)\n",
        "    temp = pd.concat([temp,temp_df],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX3cCA_dE64W"
      },
      "source": [
        "print(\"Shape of Data:\",temp.shape)\n",
        "print(\"Distinct Datatypes:\",temp.dtypes.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jFxA1cpE64X"
      },
      "source": [
        "temp[\"StateHoliday\"]= np.where(temp[\"StateHoliday\"]== '0',0,1)\n",
        "temp.dtypes.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP-_r1Z_E64X"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(temp,df_new[target],test_size = 0.2,random_state=2018)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.1,random_state=2018)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20K0nQxmE64X"
      },
      "source": [
        "## Datos del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Fpc5OfE64X"
      },
      "source": [
        "mean_sales = y_train.mean()\n",
        "print(\"Average Sales :\",mean_sales)\n",
        "print(\"MAE for Test Data:\",abs(y_test - mean_sales).mean()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-x9bj0OE64X"
      },
      "source": [
        "## NN\n",
        "\n",
        "### Rule 1: Start with small architectures\n",
        "\n",
        "In the case of DNNs, it is always advised to\n",
        "start with a single-layer network with around\n",
        "100–300 neurons. Train the network and measure\n",
        "performance using the defined metrics (while\n",
        "defining the baseline score). If the results are not\n",
        "encouraging, try adding one more layer with the\n",
        "same number of neurons and repeating the process.\n",
        "\n",
        "### Rule 2: When small architectures (with two layers) fail, increase the size\n",
        "\n",
        "When the results from small networks are not\n",
        "great, you need to increase the number of neurons\n",
        "in layers by three to five times (i.e., around 1,000\n",
        "neurons in each layer). Also, increase regularization to 0.3, 0.4,\n",
        "or 0.5 for both layers and repeat the process for\n",
        "training and performance measurement.\n",
        "\n",
        "### Rule 3: When larger networks with two layers fail, go deeper\n",
        "\n",
        "Try increasing the depth of the network with more\n",
        "and more layers while maintaining regularization with\n",
        "dropout layers (if required) after each dense (or your\n",
        "selected layer) with a dropout rate between 0.2 and 0.5.\n",
        "\n",
        "### Rule 4: When larger and deeper networks also fail, go even larger and even deeper\n",
        "\n",
        "In case large networks with ~1000 neurons and five\n",
        "or six layers also don’t give the desired performance,\n",
        "try increasing the width and depth of the network.\n",
        "Try adding layers with 8,000–10,000 neurons per\n",
        "layer and a dropout of 0.6 to 0.8.\n",
        "\n",
        "### Rule 5: When everything fails, revisit the data\n",
        "\n",
        "If all the aforementioned rules fail, revisit the\n",
        "data for improved feature engineering and\n",
        "normalization, and then you will need to try other\n",
        "ML alternatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQB2IVyoE64Y"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(150,input_dim = 44,activation=\"relu\"))\n",
        "model.add(Dense(1,activation = \"linear\"))\n",
        "model.compile(optimizer='adam',loss=\"mean_absolute_error\",metrics=[\"mean_absolute_error\"])\n",
        "model.fit(x_train.values,y_train.values, validation_data=(x_val,y_val),epochs=10,batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C7_NfnlE64Y"
      },
      "source": [
        "result = model.evaluate(x_test.values,y_test.values)\n",
        "\n",
        "for i in range(len(model.metrics_names)):\n",
        "    print(\"Metric \",model.metrics_names[i],\":\",str(round(result[i],2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4YBm7tPE64Y"
      },
      "source": [
        "### Improve Model (add layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytmx5dBNE64Y"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(150,input_dim = 44,activation=\"relu\"))\n",
        "model.add(Dense(150,activation=\"relu\"))\n",
        "model.add(Dense(150,activation=\"relu\"))\n",
        "model.add(Dense(1,activation = \"linear\"))\n",
        "model.compile(optimizer='adam',loss=\"mean_squared_error\",metrics=[\"mean_absolute_error\"])\n",
        "\n",
        "history = model.fit(x_train,y_train, validation_data=(x_val,y_val),epochs=10,batch_size=64)\n",
        "\n",
        "for i in range(len(model.metrics_names)):\n",
        "    print(\"Metric \",model.metrics_names[i],\":\",str(round(result[i],2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9aENUVzE64Z"
      },
      "source": [
        "### More Improve Model (add epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn7E0tdkE64Z"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(150,input_dim = 44,activation=\"relu\"))\n",
        "model.add(Dense(150,activation=\"relu\"))\n",
        "model.add(Dense(150,activation=\"relu\"))\n",
        "model.add(Dense(150,activation=\"relu\"))\n",
        "model.add(Dense(150,activation=\"relu\"))\n",
        "model.add(Dense(1,activation = \"linear\"))\n",
        "model.compile(optimizer='adam',loss=\"mean_squared_error\",metrics=[\"mean_absolute_error\"])\n",
        "model.fit(x_train,y_train, validation_data=(x_val,y_val),epochs=15,batch_size=64).result = model.evaluate(x_test,y_test)\n",
        "\n",
        "for i in range(len(model.metrics_names)):\n",
        "    print(\"Metric \",model.metrics_names[i],\":\",str(round(result[i],2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-30muZKE64Z"
      },
      "source": [
        "### More More Improve Model (add neurons)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr58S952E64Z"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(350,input_dim = 44,activation=\"relu\"))\n",
        "model.add(Dense(350,activation=\"relu\"))\n",
        "model.add(Dense(1,activation = \"linear\"))\n",
        "model.compile(optimizer='adam',loss=\"mean_squared_error\",metrics=[\"mean_absolute_error\"])\n",
        "model.fit(x_train,y_train, validation_data=(x_val,y_val),epochs=15,batch_size=64)\n",
        "\n",
        "result = model.evaluate(x_test,y_test)\n",
        "\n",
        "for i in range(len(model.metrics_names)):\n",
        "    print(\"Metric \",model.metrics_names[i],\":\",\n",
        "    str(round(result[i],2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2liKYaME64a"
      },
      "source": [
        "#### Let us now try deeper models for the same architecture. Additionally, we add a new optional configuration to the model to record the history of various metrics during the training process. This can be done by adding the callbacks parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3UN6suQE64a"
      },
      "source": [
        "history = History()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(350,input_dim = 44,activation=\"relu\"))\n",
        "model.add(Dense(350,activation=\"relu\"))\n",
        "model.add(Dense(350,activation=\"relu\"))\n",
        "model.add(Dense(350,activation=\"relu\"))\n",
        "model.add(Dense(350,activation=\"relu\"))\n",
        "model.add(Dense(1,activation = \"linear\"))\n",
        "model.compile(optimizer='adam',loss=\"mean_squared_error\",metrics=[\"mean_absolute_error\"])\n",
        "model.fit(x_train,y_train, validation_data=(x_val,y_val),epochs=15,batch_size=64,callbacks=[history])\n",
        "\n",
        "result = model.evaluate(x_test,y_test)\n",
        "\n",
        "for i in range(len(model.metrics_names)):\n",
        "    print(\"Metric \",model.metrics_names[i],\":\",str(round(result[i],2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AWxriZWE64a"
      },
      "source": [
        "### Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u-U9YZZE64a"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"Model's Training & Validation loss across epochs\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIMXvbC1E64a"
      },
      "source": [
        "### Checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz5R7nlfE64b"
      },
      "source": [
        "#Aquí se añade una columna, ['nombre'], por tanto ahora se tiene las columnas Sales (de antes) y Prediction\n",
        "y_test[\"Prediction\"] = model.predict(x_test)\n",
        "\n",
        "#Aquí se hace un rename de las existentes, .columns, por tanto ahora se tiene las columnas Actual Sales y Predicted Sales\n",
        "y_test.columns = [\"Actual Sales\",\"Predicted Sales\"]\n",
        "print(y_test.head(10))\n",
        "\n",
        "print(\"MSE :\",mean_squared_error(y_test[\"Actual Sales\"].values,y_test[\"Predicted Sales\"].values))\n",
        "print(\"MAE :\",mean_absolute_error(y_test[\"Actual Sales\"].values,y_test[\"Predicted Sales\"].values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxXS6HyYE64b"
      },
      "source": [
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3yNp3uyE64b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}